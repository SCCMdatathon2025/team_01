{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EN9M8Eh_mre",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "636d4e70-47a5-4357-a81a-bf9508dd1bea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: decord in /usr/local/lib/python3.11/dist-packages (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from decord) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Setup\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from google.cloud import bigquery\n",
        "import gcsfs\n",
        "import cv2\n",
        "import tempfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install decord\n",
        "from torchvision import models, transforms\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import VideoMAEForVideoClassification, pipeline\n",
        "import gcsfs\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import VideoMAEImageProcessor\n",
        "import tempfile\n",
        "from transformers import VideoMAEModel\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import decord\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noOV7wtjnExi"
      },
      "source": [
        "Add in checkpoints in case times out\n",
        "\n",
        "Optimal batch size 16-32\n",
        "\n",
        "fp16 datatype\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_VtZnVTZ8nx"
      },
      "outputs": [],
      "source": [
        "# Initialize client with project ID\n",
        "client = bigquery.Client(project='sccm-datathon-2025-participant')\n",
        "fs = gcsfs.GCSFileSystem(project='sccm-datathon-2025-participant')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0L_0Gwax0jS"
      },
      "outputs": [],
      "source": [
        "# --- 2. BigQuery data\n",
        "query_sim = \"\"\"\n",
        "SELECT sim_fileref_filename, *\n",
        "FROM `sccm-discovery.AutoDoc.Simulation Data`\n",
        "WHERE sim_fileref_filename IS NOT NULL\n",
        "\"\"\"\n",
        "\n",
        "query_ann = \"\"\"\n",
        "SELECT *\n",
        "FROM `sccm-discovery.AutoDoc.Annotations`\n",
        "\"\"\"\n",
        "\n",
        "sim_df = client.query(query_sim).to_dataframe()\n",
        "ann_df = client.query(query_ann).to_dataframe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ky1V0Yrl-BUs"
      },
      "outputs": [],
      "source": [
        "# --- 3. Merge and prepare labels\n",
        "sim_df['ad_id'] = sim_df['ad_id'].astype(str)\n",
        "ann_df['file_id'] = ann_df['file_id'].astype(str)\n",
        "merged_df = sim_df.merge(ann_df, left_on='ad_id', right_on='file_id', how='inner')\n",
        "merged_df.shape\n",
        "\n",
        "# Create multi-label table\n",
        "multi_label_df = (\n",
        "    merged_df[merged_df['task'].notna()]\n",
        "    .groupby('ad_id')['task']\n",
        "    .apply(lambda x: list(set(x)))\n",
        "    .reset_index()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df[\"skill_start_time_hh:mm:ss\"] = merged_df[\"skill_start_time_hh:mm:ss\"].astype(str)\n",
        "merged_df[\"skill_end_time_hh:mm:ss\"] = merged_df[\"skill_end_time_hh:mm:ss\"].astype(str)\n",
        "grouped = (\n",
        "    merged_df\n",
        "    .groupby('sim_fileref_filename')\n",
        "    .apply(lambda df: {\n",
        "        \"file_refname\": df.name,\n",
        "        \"annotations\": [\n",
        "            {\n",
        "                \"task\": row[\"task\"],\n",
        "                \"skill_start_time_hh:mm:ss\": row[\"skill_start_time_hh:mm:ss\"],\n",
        "                \"skill_end_time_hh:mm:ss\": row[\"skill_end_time_hh:mm:ss\"]\n",
        "            }\n",
        "            for _, row in df.iterrows()\n",
        "        ]\n",
        "    })\n",
        "    .tolist()\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YETd7qu8wUP",
        "outputId": "5309995f-5cbc-4beb-c96a-908620856306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-81-296869057.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda df: {\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-_ewvKY4_zu",
        "outputId": "f5072e50-8fce-4820-8ec4-520d0f611e4d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1442, 103)"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "# --- 3. Merge and prepare labels\n",
        "sim_df['ad_id'] = sim_df['ad_id'].astype(str)\n",
        "ann_df['file_id'] = ann_df['file_id'].astype(str)\n",
        "merged_df = sim_df.merge(ann_df, left_on='ad_id', right_on='file_id', how='inner')\n",
        "merged_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dK_3EMoZAhmK"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import decord\n",
        "from decord import VideoReader, cpu\n",
        "import torch\n",
        "import json\n",
        "import tempfile\n",
        "import os\n",
        "import re\n",
        "from torchvision import transforms\n",
        "\n",
        "decord.bridge.set_bridge(\"torch\")\n",
        "\n",
        "def download_gcs_video(gcs_uri, fs):\n",
        "    \"\"\"Downloads a video from GCS to a temporary local file.\"\"\"\n",
        "    match = re.match(r'gs://([^/]+)/(.+)', gcs_uri)\n",
        "    if not match:\n",
        "        raise ValueError(\"Invalid GCS URI\")\n",
        "\n",
        "    bucket, path = match.groups()\n",
        "\n",
        "    with fs.open(f\"{bucket}/{path}\", 'rb') as f:\n",
        "        with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as tmp:\n",
        "            tmp.write(f.read())\n",
        "            tmp.flush()\n",
        "            return tmp.name  # return local file path\n",
        "\n",
        "\n",
        "class VideoMultiLabelJSONDataset(Dataset):\n",
        "    def __init__(self, data, processor, label2id, fs, gcs_base_path=\"gs://sccm--autodoc2025/migrated_video/\"):\n",
        "        self.data = data\n",
        "        self.processor = processor\n",
        "        self.label2id = label2id\n",
        "        self.fs = fs\n",
        "        self.gcs_base_path = gcs_base_path\n",
        "        self.frames_to_select = 16 # Set the number of frames to 16\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.data[idx]\n",
        "        file_refname = entry[\"file_refname\"]\n",
        "        annotations = entry[\"annotations\"]\n",
        "\n",
        "        gcs_path = os.path.join(self.gcs_base_path, f\"{file_refname}.mp4\")\n",
        "        local_video_path = None\n",
        "\n",
        "        try:\n",
        "            # Download video\n",
        "            local_video_path = download_gcs_video(gcs_path, self.fs)\n",
        "\n",
        "            # Load video with Decord\n",
        "            vr = VideoReader(local_video_path, ctx=cpu(0))\n",
        "            num_frames = len(vr)\n",
        "\n",
        "            # Extract frames based on annotations\n",
        "            all_frames = []\n",
        "            all_labels = [0] * len(self.label2id)\n",
        "\n",
        "            # Collect frames and labels for all annotations in the video\n",
        "            frames_for_video = []\n",
        "            labels_for_video = [0] * len(self.label2id)\n",
        "\n",
        "            for annotation in annotations:\n",
        "                task = annotation[\"task\"]\n",
        "                start_time_str = annotation[\"skill_start_time_hh:mm:ss\"]\n",
        "                end_time_str = annotation[\"skill_end_time_hh:mm:ss\"]\n",
        "\n",
        "                # Convert time strings to seconds\n",
        "                def time_to_seconds(time_str):\n",
        "                    # Handle potential \"None\" values or invalid formats\n",
        "                    if time_str is None or not isinstance(time_str, str) or not re.match(r'\\d{2}:\\d{2}:\\d{2}', time_str):\n",
        "                        return -1 # Indicate invalid time\n",
        "                    h, m, s = map(int, time_str.split(':'))\n",
        "                    return h * 3600 + m * 60 + s\n",
        "\n",
        "                start_sec = time_to_seconds(start_time_str)\n",
        "                end_sec = time_to_seconds(end_time_str)\n",
        "\n",
        "                if start_sec == -1 or end_sec == -1 or start_sec >= end_sec:\n",
        "                    print(f\"Skipping invalid annotation times for video: {file_refname}, task: {task}\")\n",
        "                    continue\n",
        "\n",
        "                # Convert seconds to frame indices\n",
        "                fps = vr.get_avg_fps()\n",
        "                start_frame = int(start_sec * fps)\n",
        "                end_frame = int(end_sec * fps)\n",
        "\n",
        "                # Ensure frame indices are within bounds\n",
        "                start_frame = max(0, start_frame)\n",
        "                end_frame = min(num_frames - 1, end_frame)\n",
        "\n",
        "                # Select a fixed number of frames within the interval\n",
        "                if end_frame - start_frame + 1 < self.frames_to_select:\n",
        "                     # Not enough frames in interval, select all available and pad later\n",
        "                     indices = torch.arange(start_frame, end_frame + 1)\n",
        "                else:\n",
        "                    # Select evenly spaced frames\n",
        "                    indices = torch.linspace(start_frame, end_frame, self.frames_to_select).long()\n",
        "\n",
        "                if indices.numel() > 0:\n",
        "                    video_segment = vr.get_batch(indices)\n",
        "                    video_segment = video_segment.permute(0, 3, 1, 2).to(torch.uint8)\n",
        "                    frames_for_video.extend([transforms.ToPILImage()(video_segment[i].contiguous().cpu()) for i in range(video_segment.shape[0])])\n",
        "\n",
        "                # Update labels for this task\n",
        "                if task in self.label2id:\n",
        "                    labels_for_video[self.label2id[task]] = 1\n",
        "\n",
        "\n",
        "            if not frames_for_video:\n",
        "                 # Handle cases where no valid frames are extracted for any annotation\n",
        "                 print(f\"No valid frames extracted for video: {file_refname}. Returning dummy data.\")\n",
        "                 # Return dummy data\n",
        "                 dummy_frames = [transforms.ToPILImage()(torch.zeros(3, 224, 224, dtype=torch.uint8)) for _ in range(self.frames_to_select)] # Example dummy frames\n",
        "                 pixel_values = self.processor(dummy_frames, return_tensors=\"pt\")[\"pixel_values\"]\n",
        "                 pixel_values = pixel_values.squeeze(0)\n",
        "                 labels = torch.tensor([0] * len(self.label2id), dtype=torch.float)\n",
        "            else:\n",
        "                # Process all collected frames for the video\n",
        "                # Need to handle potential multiple segments and ensure a fixed number of frames for the model\n",
        "                # A simple approach is to sample a fixed number of frames from all collected frames\n",
        "                if len(frames_for_video) > self.frames_to_select:\n",
        "                    # Randomly sample frames if more than required\n",
        "                    sample_indices = torch.randperm(len(frames_for_video))[:self.frames_to_select]\n",
        "                    sampled_frames = [frames_for_video[i] for i in sample_indices]\n",
        "                else:\n",
        "                    # Use all collected frames and pad if less than required\n",
        "                    sampled_frames = frames_for_video\n",
        "                    while len(sampled_frames) < self.frames_to_select:\n",
        "                         # Pad with dummy frames\n",
        "                         sampled_frames.append(transforms.ToPILImage()(torch.zeros(3, 224, 224, dtype=torch.uint8)))\n",
        "\n",
        "\n",
        "                pixel_values = self.processor(sampled_frames, return_tensors=\"pt\")[\"pixel_values\"]\n",
        "                pixel_values = pixel_values.squeeze(0) # Remove batch dimension\n",
        "\n",
        "                labels = torch.tensor(labels_for_video, dtype=torch.float)\n",
        "\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Skipping missing video file during processing: {gcs_path}\")\n",
        "            # Return dummy data to avoid crashing the training loop\n",
        "            dummy_frames = [transforms.ToPILImage()(torch.zeros(3, 224, 224, dtype=torch.uint8)) for _ in range(self.frames_to_select)] # Example dummy frames\n",
        "            pixel_values = self.processor(dummy_frames, return_tensors=\"pt\")[\"pixel_values\"]\n",
        "            pixel_values = pixel_values.squeeze(0)\n",
        "            labels = torch.tensor([0] * len(self.label2id), dtype=torch.float)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while processing video {file_refname}: {e}\")\n",
        "            # Return dummy data or raise the exception depending on desired behavior\n",
        "            dummy_frames = [transforms.ToPILImage()(torch.zeros(3, 224, 224, dtype=torch.uint8)) for _ in range(self.frames_to_select)] # Example dummy frames\n",
        "            pixel_values = self.processor(dummy_frames, return_tensors=\"pt\")[\"pixel_values\"]\n",
        "            pixel_values = pixel_values.squeeze(0)\n",
        "            labels = torch.tensor([0] * len(self.label2id), dtype=torch.float)\n",
        "\n",
        "\n",
        "        finally:\n",
        "            # Clean up the temporary local file\n",
        "            if local_video_path and os.path.exists(local_video_path):\n",
        "                os.remove(local_video_path)\n",
        "\n",
        "\n",
        "        return {\"pixel_values\": pixel_values, \"labels\": labels}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "weHynIOF-LQM"
      },
      "outputs": [],
      "source": [
        "# --- 4. Multiclass\n",
        "\n",
        "# class VideoMultiLabelDataset(Dataset):\n",
        "#     def __init__(self, df, processor, fs, label_columns):\n",
        "#         self.df = df.reset_index(drop=True)  # Ensure index is clean\n",
        "#         self.processor = processor\n",
        "#         self.fs = fs\n",
        "#         self.label_columns = label_columns\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.df)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         import decord\n",
        "#         from decord import VideoReader, cpu\n",
        "#         decord.bridge.set_bridge(\"torch\")\n",
        "\n",
        "#         for _ in range(5):  # Try a few times to find a valid sample\n",
        "#             try:\n",
        "#                 row = self.df.iloc[idx]\n",
        "#                 gcs_path = f\"gs://sccm--autodoc2025/migrated_video/{row['sim_fileref_filename']}.mp4\"\n",
        "#                 local_path = download_gcs_video(gcs_path)\n",
        "\n",
        "#                 vr = VideoReader(local_path, ctx=cpu(0))\n",
        "#                 num_frames = 32\n",
        "#                 indices = torch.linspace(0, len(vr) - 1, num_frames).long()\n",
        "#                 # Get video frames from Decord\n",
        "#                 video = vr.get_batch(indices)\n",
        "#                 video = video.permute(0, 3, 1, 2).to(torch.uint8)\n",
        "\n",
        "#                 # Convert each frame to PIL\n",
        "#                 frames = [transforms.ToPILImage()(video[i].contiguous().cpu()) for i in range(video.shape[0])]\n",
        "\n",
        "#                 # Pass PIL frames to processor\n",
        "#                 pixel_values = self.processor(frames, return_tensors=\"pt\")[\"pixel_values\"]\n",
        "\n",
        "\n",
        "#                 # Remove batch dim if needed\n",
        "#                 pixel_values = pixel_values.squeeze(0)  # shape: (num_frames, C, H, W)\n",
        "\n",
        "#                 # Labels\n",
        "#                 labels = torch.tensor(row[self.label_columns].values.astype(float), dtype=torch.float)\n",
        "\n",
        "#                 return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
        "\n",
        "\n",
        "#             except FileNotFoundError:\n",
        "#                 print(f\"Skipping missing video: {row['sim_fileref_filename']}\")\n",
        "#                 idx = (idx + 1) % len(self.df)  # try next sample\n",
        "\n",
        "#         raise RuntimeError(\"Too many missing videos in a row\")\n",
        "\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "label_matrix = mlb.fit_transform(multi_label_df['task'])\n",
        "label_columns = mlb.classes_\n",
        "\n",
        "multi_label_df = multi_label_df.join(pd.DataFrame(label_matrix, columns=label_columns))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UR4pJPjEBPi1"
      },
      "outputs": [],
      "source": [
        "label2id = {label: i for i, label in enumerate(label_columns)}\n",
        "id2label = {i: label for label, i in label2id.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHAsbT2l_8yA"
      },
      "outputs": [],
      "source": [
        "# --- 5. Merge labels with simulation data (to get video path info)\n",
        "full_df = sim_df.merge(multi_label_df, on='ad_id', how='inner')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "eIoOCMZcBUb6",
        "outputId": "b3fb2cb6-1d37-4bb1-902e-c69d2d8c302d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   sim_fileref_filename ad_id redcap_repeat_instrument  \\\n",
              "0  44e44a8b-3a17-4c36-bca9-14d3a5b26358     1           file_reference   \n",
              "1  ca36e521-3e9b-4bcd-9f06-579b361ecf8a     2           file_reference   \n",
              "2  f628cf42-cbde-4973-993e-8f2c45eaadc9     3           file_reference   \n",
              "3  31e5ca7d-67d1-42bc-9699-a975f860c997     4           file_reference   \n",
              "4  e03d79a0-f71b-4db8-9cec-76542d472cb8     5           file_reference   \n",
              "\n",
              "   redcap_repeat_instance  sim_migrated sim_migrated_name  sim_mascal  \\\n",
              "0                       1          <NA>              None        <NA>   \n",
              "1                       1          <NA>              None        <NA>   \n",
              "2                       1          <NA>              None        <NA>   \n",
              "3                       1          <NA>              None        <NA>   \n",
              "4                       1          <NA>              None        <NA>   \n",
              "\n",
              "  ad_starttime ad_endtime  ad_usecontext  ...  Nasopharyngeal Airway  \\\n",
              "0         None       None           <NA>  ...                      1   \n",
              "1         None       None           <NA>  ...                      1   \n",
              "2         None       None           <NA>  ...                      1   \n",
              "3         None       None           <NA>  ...                      1   \n",
              "4         None       None           <NA>  ...                      0   \n",
              "\n",
              "   Pressure Dressing Application  StartEx  Surgical cricothyrotomy   \\\n",
              "0                              1        1                         0   \n",
              "1                              1        1                         0   \n",
              "2                              1        1                         0   \n",
              "3                              1        1                         0   \n",
              "4                              1        1                         0   \n",
              "\n",
              "   Time Sync  Tourniquet Application  Treat a Casualty for a Cold Injury  \\\n",
              "0          1                       0                                   1   \n",
              "1          1                       0                                   1   \n",
              "2          1                       0                                   0   \n",
              "3          1                       1                                   0   \n",
              "4          1                       1                                   0   \n",
              "\n",
              "   Treat a Casualty with Burns  Treat a Casualty with a Pelvic Fracture  \\\n",
              "0                            0                                        0   \n",
              "1                            0                                        0   \n",
              "2                            0                                        0   \n",
              "3                            0                                        1   \n",
              "4                            0                                        1   \n",
              "\n",
              "  Uses Sensor  \n",
              "0           1  \n",
              "1           0  \n",
              "2           0  \n",
              "3           0  \n",
              "4           0  \n",
              "\n",
              "[5 rows x 109 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4ced5189-f7af-4cd5-b4a0-09fe6f237e70\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sim_fileref_filename</th>\n",
              "      <th>ad_id</th>\n",
              "      <th>redcap_repeat_instrument</th>\n",
              "      <th>redcap_repeat_instance</th>\n",
              "      <th>sim_migrated</th>\n",
              "      <th>sim_migrated_name</th>\n",
              "      <th>sim_mascal</th>\n",
              "      <th>ad_starttime</th>\n",
              "      <th>ad_endtime</th>\n",
              "      <th>ad_usecontext</th>\n",
              "      <th>...</th>\n",
              "      <th>Nasopharyngeal Airway</th>\n",
              "      <th>Pressure Dressing Application</th>\n",
              "      <th>StartEx</th>\n",
              "      <th>Surgical cricothyrotomy</th>\n",
              "      <th>Time Sync</th>\n",
              "      <th>Tourniquet Application</th>\n",
              "      <th>Treat a Casualty for a Cold Injury</th>\n",
              "      <th>Treat a Casualty with Burns</th>\n",
              "      <th>Treat a Casualty with a Pelvic Fracture</th>\n",
              "      <th>Uses Sensor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>44e44a8b-3a17-4c36-bca9-14d3a5b26358</td>\n",
              "      <td>1</td>\n",
              "      <td>file_reference</td>\n",
              "      <td>1</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>None</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ca36e521-3e9b-4bcd-9f06-579b361ecf8a</td>\n",
              "      <td>2</td>\n",
              "      <td>file_reference</td>\n",
              "      <td>1</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>None</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>f628cf42-cbde-4973-993e-8f2c45eaadc9</td>\n",
              "      <td>3</td>\n",
              "      <td>file_reference</td>\n",
              "      <td>1</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>None</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>31e5ca7d-67d1-42bc-9699-a975f860c997</td>\n",
              "      <td>4</td>\n",
              "      <td>file_reference</td>\n",
              "      <td>1</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>None</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>e03d79a0-f71b-4db8-9cec-76542d472cb8</td>\n",
              "      <td>5</td>\n",
              "      <td>file_reference</td>\n",
              "      <td>1</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>None</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 109 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4ced5189-f7af-4cd5-b4a0-09fe6f237e70')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4ced5189-f7af-4cd5-b4a0-09fe6f237e70 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4ced5189-f7af-4cd5-b4a0-09fe6f237e70');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-afe14e53-50aa-40e9-88b5-2f95c5fa2e6b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-afe14e53-50aa-40e9-88b5-2f95c5fa2e6b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-afe14e53-50aa-40e9-88b5-2f95c5fa2e6b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "full_df"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "full_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8YBNNPoEFhB"
      },
      "outputs": [],
      "source": [
        "def download_gcs_video(gcs_uri, fs):\n",
        "    # gcs_uri = 'gs://bucket_name/path/to/file.mp4'\n",
        "    import re\n",
        "    match = re.match(r'gs://([^/]+)/(.+)', gcs_uri)\n",
        "    if not match:\n",
        "        raise ValueError(\"Invalid GCS URI\")\n",
        "\n",
        "    bucket, path = match.groups()\n",
        "    # fs = gcsfs.GCSFileSystem(project='sccm-datathon-2025-participant') # fs is now passed as an argument\n",
        "\n",
        "    with fs.open(f\"{bucket}/{path}\", 'rb') as f:\n",
        "        with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as tmp:\n",
        "            tmp.write(f.read())\n",
        "            tmp.flush()\n",
        "            return tmp.name  # return local file path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qamiuCaHLXEJ"
      },
      "outputs": [],
      "source": [
        "def multilabel_collate_fn(batch):\n",
        "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
        "    labels = torch.stack([item[\"labels\"] for item in batch])\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvXJ5L5vMCYt"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Count label occurrences\n",
        "label_counts = full_df[label_columns].sum()\n",
        "\n",
        "# Keep only labels that appear at least 2 times\n",
        "valid_labels = label_counts[label_counts >= 2].index.tolist()\n",
        "\n",
        "# Filter full_df and label columns\n",
        "filtered_df = full_df[full_df[valid_labels].sum(axis=1) > 0].copy()\n",
        "\n",
        "def video_exists(file_id):\n",
        "    gcs_path = f\"sccm--autodoc2025/migrated_video/{file_id}.mp4\"\n",
        "    return fs.exists(gcs_path)\n",
        "\n",
        "# Apply check\n",
        "filtered_df['exists'] = filtered_df['sim_fileref_filename'].apply(video_exists)\n",
        "\n",
        "# Keep only rows with existing videos\n",
        "filtered_df = filtered_df[filtered_df['exists']]\n",
        "\n",
        "aligned_label_matrix = filtered_df[valid_labels].values\n",
        "\n",
        "subset_df = filtered_df.sample(n=10, random_state=42)\n",
        "train_df, val_df = train_test_split(subset_df, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSz8F8cMLmlM"
      },
      "outputs": [],
      "source": [
        "# from transformers import VideoMAEModel\n",
        "# import torch.nn as nn\n",
        "\n",
        "# class MultiLabelVideoMAE(torch.nn.Module):\n",
        "#     def __init__(self, num_labels, label2id, id2label):\n",
        "#         super().__init__()\n",
        "#         self.backbone = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\")\n",
        "#         hidden_size = self.backbone.config.hidden_size\n",
        "#         self.classifier = nn.Linear(hidden_size, num_labels)\n",
        "\n",
        "#         # Update config\n",
        "#         self.config = self.backbone.config\n",
        "#         self.config.num_labels = num_labels\n",
        "#         self.config.label2id = label2id\n",
        "#         self.config.id2label = id2label\n",
        "\n",
        "#     def forward(self, pixel_values, labels=None):\n",
        "#         outputs = self.backbone(pixel_values=pixel_values)\n",
        "#         cls_token = outputs.last_hidden_state[:, 0]  # (batch_size, hidden_size)\n",
        "#         logits = self.classifier(cls_token)\n",
        "\n",
        "#         if labels is not None:\n",
        "#             loss_fn = nn.BCEWithLogitsLoss()\n",
        "#             loss = loss_fn(logits, labels.float())\n",
        "#             return {\"loss\": loss, \"logits\": logits}\n",
        "#         return {\"logits\": logits}\n",
        "\n",
        "# processor = VideoMAEImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
        "\n",
        "# label_columns = valid_labels\n",
        "# label2id = {label: i for i, label in enumerate(label_columns)}\n",
        "# id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "# train_dataset = VideoMultiLabelDataset(train_df, processor, fs, label_columns)\n",
        "# val_dataset = VideoMultiLabelDataset(val_df, processor, fs, label_columns)\n",
        "\n",
        "# args = TrainingArguments(\n",
        "#     output_dir=\"./videomae-multilabel\",\n",
        "#     save_strategy=\"epoch\",\n",
        "#     eval_strategy=\"epoch\",\n",
        "#     per_device_train_batch_size=2,\n",
        "#     per_device_eval_batch_size=2,\n",
        "#     num_train_epochs=5,\n",
        "#     learning_rate=2e-5,\n",
        "#     remove_unused_columns=False,\n",
        "#     logging_steps=10,\n",
        "# )\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model=MultiLabelVideoMAE(num_labels=len(label_columns), label2id=label2id, id2label=id2label),\n",
        "#     args=args,\n",
        "#     train_dataset=train_dataset,\n",
        "#     eval_dataset=val_dataset,\n",
        "#     data_collator=multilabel_collate_fn,\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0g8RU17HmNag",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1860cf5c-fb70-478b-905f-2e9ad0051e7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "ðŸš€ Starting training with Google Drive checkpoints...\n",
            "\n",
            "ðŸ“ˆ Starting Epoch 1/5\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/e68b7fd3-1581-43bb-b744-52b0421d49dc.mp4\n",
            "Skipping invalid annotation times for video: 65fa3944-ed6c-4ae7-9f96-5df76b0396f6, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/f2b4b63e-d0dd-43ef-ba96-6824802510b5.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/7a42c3ab-c927-432e-8815-cd03aec2aa9d.mp4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='78' max='385' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 78/385 1:08:25 < 4:36:22, 0.02 it/s, Epoch 1/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='45' max='77' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [45/77 34:56 < 25:24, 0.02 it/s]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping invalid annotation times for video: 0eb95908-f038-4ac2-bcee-1ab7b4ab4068, task: Time Sync\n",
            "Skipping invalid annotation times for video: 31931d51-7792-4d6b-b859-14e7562b7879, task: Time Sync\n",
            "Skipping invalid annotation times for video: ec7538e7-1813-4f3f-b76a-1667453caba9, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/8f6e91f1-cfda-46a0-ba46-6f2004b49c6b.mp4\n",
            "Skipping invalid annotation times for video: f628cf42-cbde-4973-993e-8f2c45eaadc9, task: Time Sync\n",
            "Skipping invalid annotation times for video: 79aeb0ba-04c7-40a6-b033-939c002c2c9a, task: Time Sync\n",
            "Skipping invalid annotation times for video: 6a81d79f-f45a-4362-af09-946c423553a6, task: Time Sync\n",
            "Skipping invalid annotation times for video: ff42a428-999e-48ce-ad97-14eedd580395, task: Time Sync\n",
            "Skipping invalid annotation times for video: a0dc9042-28b9-4452-9a82-f04ba5d5e0f9, task: Time Sync\n",
            "Skipping invalid annotation times for video: 25eab334-1a93-48db-82be-6f2a6003750a, task: Time Sync\n",
            "Skipping invalid annotation times for video: ed38cf9e-4235-4227-a142-1aa4efae97fa, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/d0738d43-908d-40cd-9123-35db0846f7ff.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/f16663d6-1b31-421e-887d-5fc1245ded82.mp4\n",
            "Skipping invalid annotation times for video: f9b2de1d-0a4a-41f1-99ba-1ca2527e9db6, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/346a0c11-1d88-428d-96e5-bde642927f34.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/005f139b-5b8b-4313-a26b-c284754f1fde.mp4\n",
            "Skipping invalid annotation times for video: bf74e034-f6b4-4022-8f16-562c73e99998, task: Time Sync\n",
            "Skipping invalid annotation times for video: 6babc665-ce3b-401b-9467-c7c3e68db029, task: Time Sync\n",
            "Skipping invalid annotation times for video: 33a52f51-7095-4486-9665-25b1a1cfa82d, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/de2fcf5d-97da-4678-8ac6-927d3c024060.mp4\n",
            "Skipping invalid annotation times for video: 7974cb39-d212-43fd-ab3b-9ed44c94641d, task: Time Sync\n",
            "Skipping invalid annotation times for video: 819550a5-81f0-4665-8910-e7ee81430f91, task: Time Sync\n",
            "Skipping invalid annotation times for video: 819550a5-81f0-4665-8910-e7ee81430f91, task: Uses Sensor\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/98c98697-1118-4cbc-b98c-5499fe3248b6.mp4\n",
            "Skipping invalid annotation times for video: 951e473b-60c5-40fe-b47e-8265b614df1a, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/d52ef21c-2918-444d-a364-820d92eef510.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/bb2ae2db-1274-4e84-a6ef-eff7cc9c6930.mp4\n",
            "Skipping invalid annotation times for video: e2920b32-fbe9-49a6-92ca-0ac01ccd2960, task: Time Sync\n",
            "An error occurred while processing video e2920b32-fbe9-49a6-92ca-0ac01ccd2960: upper bound and larger bound inconsistent with step sign\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/d2d79898-f6c0-414e-b060-d6cd6298c692.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/d598ef53-7e18-4415-be5b-4260ba50ed26.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/3dd80435-4fd4-441e-b235-3ae33188ea38.mp4\n",
            "Skipping invalid annotation times for video: 28cf8d37-1ef1-4fc4-b87e-1d2805748cde, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/d477c7b9-46b3-4186-a17c-38fe03e4f546.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/fd10f3cd-2642-42d4-a3ea-3e699a0c682a.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/04634223-6737-48a7-b8b7-83a941c91539.mp4\n",
            "Skipping invalid annotation times for video: 3f058d96-18f4-4a5b-b869-e7b90acbc1f9, task: Time Sync\n",
            "Skipping invalid annotation times for video: 04f427d0-08a9-4ef7-8304-f8e3964c168a, task: Time Sync\n",
            "Skipping invalid annotation times for video: f2b6dc49-320f-436f-9c27-26dcb875eee2, task: Time Sync\n",
            "Skipping invalid annotation times for video: 646fc943-0db9-4b15-b2c7-e8f8ce589dd8, task: Time Sync\n",
            "Skipping invalid annotation times for video: ca36e521-3e9b-4bcd-9f06-579b361ecf8a, task: Time Sync\n",
            "Skipping invalid annotation times for video: a765f8c8-ef16-4158-9300-5f10c5c6c6d3, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/e6cdad8d-094c-4350-8daf-9153f4578919.mp4\n",
            "Skipping invalid annotation times for video: a668ff03-81db-41a3-90b1-4e3a34948e77, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/d138001f-0f57-4d77-b997-aa7365121654.mp4\n",
            "Skipping invalid annotation times for video: 6eb1022b-b023-46b6-b08f-71b21528f7fd, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/4fa26886-c84d-42f5-9879-0558585070fb.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/7e59793f-0365-4f10-af62-a5d6706bb060.mp4\n",
            "Skipping invalid annotation times for video: e03d79a0-f71b-4db8-9cec-76542d472cb8, task: Time Sync\n",
            "Skipping invalid annotation times for video: d607f689-47e4-41eb-b500-9a742714ee67, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/65ee21b9-83b5-4d66-bfb3-7bd723f36c9d.mp4\n",
            "Skipping invalid annotation times for video: 31e5ca7d-67d1-42bc-9699-a975f860c997, task: Time Sync\n",
            "Skipping invalid annotation times for video: 04e1e674-1302-4bf4-8d1c-be6091edc3f6, task: Time Sync\n",
            "Skipping invalid annotation times for video: 15d26949-e2b9-4dbc-834d-7c30239f2b7e, task: Time Sync\n",
            "Skipping invalid annotation times for video: 5172fcaa-96af-4c3a-b952-1714c8b1f61b, task: Time Sync\n",
            "Skipping invalid annotation times for video: bab575ab-b397-4cdd-ab11-6b8be328d7f8, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/ed89b36f-336d-4433-a002-60b820210da0.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/a2651681-5280-4292-b33b-ddc4fe56078a.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/4c80c9e8-f841-4e8b-8a86-e30734f533a2.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/0e5adeaa-fecb-42a1-983a-5429f7e6a923.mp4\n",
            "Skipping invalid annotation times for video: d99f8c51-7c69-432d-a000-fdb4cf3f36c9, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/e24aa8b7-e095-4554-97c5-74bc0a5b7536.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/9e967f16-f69b-402e-b506-f1f82448d70b.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/bb5f2e18-42c4-4c17-8245-2aa37191af67.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/24d3c64c-cc53-4743-b0cc-2558d5dc90b5.mp4\n",
            "Skipping invalid annotation times for video: fb543b2a-b97b-4969-8495-796722c26014, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/ee8f0f67-4811-4ed4-9284-3f52a7cff7a6.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/08637b58-0752-4d0e-a2cd-fecc9aa6f738.mp4\n",
            "Skipping invalid annotation times for video: e2a53599-1fb9-456a-a26f-c655bc21016f, task: Time Sync\n",
            "Skipping invalid annotation times for video: e4e02d1e-0caa-41ac-bd97-fb52fc065bc8, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/77c66203-9fba-490b-90f5-2dabdaad8f51.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/44e44a8b-3a17-4c36-bca9-14d3a5b26358.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/78446be5-b386-4450-82e8-c101dd660a5a.mp4\n",
            "Skipping invalid annotation times for video: ec9c3b14-87e6-4fd4-84a7-c9eca889df8b, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/72789f05-8899-4d06-bd17-18f7f74bd25b.mp4\n",
            "Skipping invalid annotation times for video: 0ea38263-f6ad-4f6a-8152-027572e1c77c, task: Time Sync\n",
            "Skipping invalid annotation times for video: 50c4bfd0-a0ee-4aa7-ad9e-b4ba71e9dac9, task: Time Sync\n",
            "Skipping invalid annotation times for video: e9b2d164-7c99-460f-aaaf-79260a2d3173, task: Time Sync\n",
            "Skipping invalid annotation times for video: dc1ff80e-c86e-4b55-8aa1-e6ca9039523c, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/80b7cd8d-9db8-4563-a731-afb5efdab3f2.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/55e3c203-59d0-4aa9-bf44-4a262466edb2.mp4\n",
            "Skipping invalid annotation times for video: ddde396f-b137-4c33-a134-634e48b22a4e, task: Time Sync\n",
            "Skipping invalid annotation times for video: a31a721d-95c4-4f41-aae0-c676df0ba41f, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/03112191-817d-479e-9664-c4a4c8ded3a2.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/8bf3d41d-7c6e-4539-b2ea-d8ffaa99559b.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/ff06f086-a606-4889-8da0-8572fcc2a742.mp4\n",
            "Skipping invalid annotation times for video: d1935fb2-7217-45a7-aaef-1e690d192bd5, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/7e5aa195-db53-4f43-bcc7-7eb1a5fe2c4c.mp4\n",
            "Skipping invalid annotation times for video: 81acfb0e-9197-4252-9673-117928264e36, task: Time Sync\n",
            "Skipping invalid annotation times for video: 6d292ba6-d9a6-4467-8bb2-da783ac1d0b2, task: Time Sync\n",
            "Skipping invalid annotation times for video: a8ed283f-1fea-439c-aef1-7191e464dd8d, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/cd7f1358-0725-4505-8b73-165e29d31645.mp4\n",
            "Skipping invalid annotation times for video: b446a28c-abf9-4830-90f0-e61fff423269, task: StartEx\n",
            "Skipping invalid annotation times for video: e6dc65b6-b169-43d1-ad53-227dea413373, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/38b6b0b8-b337-42e7-a008-7fffbb53dbbd.mp4\n",
            "Skipping invalid annotation times for video: 7040d1cf-3132-4da4-92d0-e49c0736f37e, task: Time Sync\n",
            "Skipping invalid annotation times for video: 7040d1cf-3132-4da4-92d0-e49c0736f37e, task: ENDEX\n",
            "Skipping invalid annotation times for video: 2672fe85-7848-462f-9205-5b3146b0d725, task: Time Sync\n",
            "Skipping invalid annotation times for video: f1417f4a-5470-4eb2-8922-36ccc965e03e, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/11b8af4f-aff0-4f9d-9bf3-14415689c18a.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/9e8614bd-3d2c-445d-88a7-d4267ab3fbf4.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/c9d717e3-0e7c-4de9-96c6-6639483c2fe6.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/33d8f786-e5b1-4e13-b12f-420d3d3afc40.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/b394392f-6615-4396-a7d0-fbdd695cff03.mp4\n",
            "Skipping invalid annotation times for video: 55d8bf6b-a519-4b27-bbe0-12eb91e237e1, task: Time Sync\n",
            "Skipping invalid annotation times for video: 55d8bf6b-a519-4b27-bbe0-12eb91e237e1, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/35ca5aae-8d8d-4edb-acc6-2994ba2510bc.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/91df6b0e-a91c-494f-a041-54b674ec4090.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/18522fb8-2317-450b-bdca-f73396cab97a.mp4\n",
            "Skipping invalid annotation times for video: 43f74612-36de-4165-b0fe-1cb781afc865, task: Time Sync\n",
            "Skipping invalid annotation times for video: c459fb41-d56c-465f-a711-a152849c121c, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/aee3287f-377e-4f17-81c4-983e81d9c5b7.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/650c7d80-e4c3-4f55-bddf-e8f62568828d.mp4\n",
            "Skipping invalid annotation times for video: dd960155-4997-412a-adc0-d13f15a6301e, task: Time Sync\n",
            "Skipping invalid annotation times for video: 8ae7e137-89b1-45a2-a566-39a7a93b7ed8, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/c9aff860-f198-4a07-bd49-96ab7098983e.mp4\n",
            "Skipping invalid annotation times for video: 99ad1759-e687-4031-a211-09e0dc32b848, task: Time Sync\n",
            "Skipping invalid annotation times for video: 119dc5bc-dbb8-4624-a16a-e9fd0dcf22b5, task: Time Sync\n",
            "Skipping invalid annotation times for video: 07aaf6f6-ce06-4aad-b757-0588addbab8f, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/bf157260-1c36-4c9c-81b5-0849346de608.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/0d5379de-2ddc-42d2-ad4c-a1f101740f27.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/5563925f-e651-4452-8893-f34ebbbab18f.mp4\n",
            "Skipping invalid annotation times for video: dd969049-7d81-4127-98f3-37f81f613127, task: Time Sync\n",
            "Skipping invalid annotation times for video: adf5d6ae-00f9-485c-8577-debe2be6c27d, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/ea5e36be-a22e-4ead-a44c-bd2cfc4a9626.csv.mp4\n",
            "Skipping invalid annotation times for video: 87fae222-4018-46c7-98ff-4a7186ad8d2b, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/44bc6d98-6b1b-4572-b64c-78d14b02d741.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/0409044b-4bf9-4ae7-9e0f-1e421abcf0d2.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/005f139b-5b8b-4313-a26b-c284754f1fde.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/03112191-817d-479e-9664-c4a4c8ded3a2.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/0409044b-4bf9-4ae7-9e0f-1e421abcf0d2.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/04634223-6737-48a7-b8b7-83a941c91539.mp4\n",
            "Skipping invalid annotation times for video: 04e1e674-1302-4bf4-8d1c-be6091edc3f6, task: Time Sync\n",
            "Skipping invalid annotation times for video: 04f427d0-08a9-4ef7-8304-f8e3964c168a, task: Time Sync\n",
            "Skipping invalid annotation times for video: 07aaf6f6-ce06-4aad-b757-0588addbab8f, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/08637b58-0752-4d0e-a2cd-fecc9aa6f738.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/0d5379de-2ddc-42d2-ad4c-a1f101740f27.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/0e5adeaa-fecb-42a1-983a-5429f7e6a923.mp4\n",
            "Skipping invalid annotation times for video: 0ea38263-f6ad-4f6a-8152-027572e1c77c, task: Time Sync\n",
            "Skipping invalid annotation times for video: 0eb95908-f038-4ac2-bcee-1ab7b4ab4068, task: Time Sync\n",
            "Skipping invalid annotation times for video: 119dc5bc-dbb8-4624-a16a-e9fd0dcf22b5, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/11b8af4f-aff0-4f9d-9bf3-14415689c18a.mp4\n",
            "Skipping invalid annotation times for video: 15d26949-e2b9-4dbc-834d-7c30239f2b7e, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/18522fb8-2317-450b-bdca-f73396cab97a.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/24d3c64c-cc53-4743-b0cc-2558d5dc90b5.mp4\n",
            "Skipping invalid annotation times for video: 25eab334-1a93-48db-82be-6f2a6003750a, task: Time Sync\n",
            "Skipping invalid annotation times for video: 2672fe85-7848-462f-9205-5b3146b0d725, task: Time Sync\n",
            "Skipping invalid annotation times for video: 28cf8d37-1ef1-4fc4-b87e-1d2805748cde, task: Time Sync\n",
            "Skipping invalid annotation times for video: 31931d51-7792-4d6b-b859-14e7562b7879, task: Time Sync\n",
            "Skipping invalid annotation times for video: 31e5ca7d-67d1-42bc-9699-a975f860c997, task: Time Sync\n",
            "Skipping invalid annotation times for video: 33a52f51-7095-4486-9665-25b1a1cfa82d, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/33d8f786-e5b1-4e13-b12f-420d3d3afc40.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/346a0c11-1d88-428d-96e5-bde642927f34.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/35ca5aae-8d8d-4edb-acc6-2994ba2510bc.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/38b6b0b8-b337-42e7-a008-7fffbb53dbbd.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/3dd80435-4fd4-441e-b235-3ae33188ea38.mp4\n",
            "Skipping invalid annotation times for video: 3f058d96-18f4-4a5b-b869-e7b90acbc1f9, task: Time Sync\n",
            "Skipping invalid annotation times for video: 43f74612-36de-4165-b0fe-1cb781afc865, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/44bc6d98-6b1b-4572-b64c-78d14b02d741.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/44e44a8b-3a17-4c36-bca9-14d3a5b26358.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/4c80c9e8-f841-4e8b-8a86-e30734f533a2.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/4fa26886-c84d-42f5-9879-0558585070fb.mp4\n",
            "Skipping invalid annotation times for video: 50c4bfd0-a0ee-4aa7-ad9e-b4ba71e9dac9, task: Time Sync\n",
            "Skipping invalid annotation times for video: 5172fcaa-96af-4c3a-b952-1714c8b1f61b, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/5563925f-e651-4452-8893-f34ebbbab18f.mp4\n",
            "Skipping invalid annotation times for video: 55d8bf6b-a519-4b27-bbe0-12eb91e237e1, task: Time Sync\n",
            "Skipping invalid annotation times for video: 55d8bf6b-a519-4b27-bbe0-12eb91e237e1, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/55e3c203-59d0-4aa9-bf44-4a262466edb2.mp4\n",
            "Skipping invalid annotation times for video: 646fc943-0db9-4b15-b2c7-e8f8ce589dd8, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/650c7d80-e4c3-4f55-bddf-e8f62568828d.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/65ee21b9-83b5-4d66-bfb3-7bd723f36c9d.mp4\n",
            "Skipping invalid annotation times for video: 65fa3944-ed6c-4ae7-9f96-5df76b0396f6, task: Time Sync\n",
            "Skipping invalid annotation times for video: 6a81d79f-f45a-4362-af09-946c423553a6, task: Time Sync\n",
            "Skipping invalid annotation times for video: 6babc665-ce3b-401b-9467-c7c3e68db029, task: Time Sync\n",
            "Skipping invalid annotation times for video: 6d292ba6-d9a6-4467-8bb2-da783ac1d0b2, task: Time Sync\n",
            "Skipping invalid annotation times for video: 6eb1022b-b023-46b6-b08f-71b21528f7fd, task: Time Sync\n",
            "Skipping invalid annotation times for video: 7040d1cf-3132-4da4-92d0-e49c0736f37e, task: Time Sync\n",
            "Skipping invalid annotation times for video: 7040d1cf-3132-4da4-92d0-e49c0736f37e, task: ENDEX\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/72789f05-8899-4d06-bd17-18f7f74bd25b.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/77c66203-9fba-490b-90f5-2dabdaad8f51.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/78446be5-b386-4450-82e8-c101dd660a5a.mp4\n",
            "Skipping invalid annotation times for video: 7974cb39-d212-43fd-ab3b-9ed44c94641d, task: Time Sync\n",
            "Skipping invalid annotation times for video: 79aeb0ba-04c7-40a6-b033-939c002c2c9a, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/7a42c3ab-c927-432e-8815-cd03aec2aa9d.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/7e59793f-0365-4f10-af62-a5d6706bb060.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/7e5aa195-db53-4f43-bcc7-7eb1a5fe2c4c.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/80b7cd8d-9db8-4563-a731-afb5efdab3f2.mp4\n",
            "Skipping invalid annotation times for video: 819550a5-81f0-4665-8910-e7ee81430f91, task: Time Sync\n",
            "Skipping invalid annotation times for video: 819550a5-81f0-4665-8910-e7ee81430f91, task: Uses Sensor\n",
            "Skipping invalid annotation times for video: 81acfb0e-9197-4252-9673-117928264e36, task: Time Sync\n",
            "Skipping invalid annotation times for video: 87fae222-4018-46c7-98ff-4a7186ad8d2b, task: Time Sync\n",
            "Skipping invalid annotation times for video: 8ae7e137-89b1-45a2-a566-39a7a93b7ed8, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/8bf3d41d-7c6e-4539-b2ea-d8ffaa99559b.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/8f6e91f1-cfda-46a0-ba46-6f2004b49c6b.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/91df6b0e-a91c-494f-a041-54b674ec4090.mp4\n",
            "Skipping invalid annotation times for video: 951e473b-60c5-40fe-b47e-8265b614df1a, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/98c98697-1118-4cbc-b98c-5499fe3248b6.mp4\n",
            "Skipping invalid annotation times for video: 99ad1759-e687-4031-a211-09e0dc32b848, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/9e8614bd-3d2c-445d-88a7-d4267ab3fbf4.mp4\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/9e967f16-f69b-402e-b506-f1f82448d70b.mp4\n",
            "Skipping invalid annotation times for video: a0dc9042-28b9-4452-9a82-f04ba5d5e0f9, task: Time Sync\n",
            "Skipping missing video file during processing: gs://sccm--autodoc2025/migrated_video/a2651681-5280-4292-b33b-ddc4fe56078a.mp4\n",
            "Skipping invalid annotation times for video: a31a721d-95c4-4f41-aae0-c676df0ba41f, task: Time Sync\n",
            "Skipping invalid annotation times for video: a668ff03-81db-41a3-90b1-4e3a34948e77, task: Time Sync\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-98-2259970053.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;31m# Setup and run training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_training_with_checkpoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0mcustom_training_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ðŸŽ‰ Training completed! All checkpoints saved to Google Drive.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-98-2259970053.py\u001b[0m in \u001b[0;36mcustom_training_loop\u001b[0;34m(trainer, checkpoint_callback)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m# Train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m# Evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2204\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2206\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2207\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m             self._maybe_log_save_evaluate(\n\u001b[0m\u001b[1;32m   2658\u001b[0m                 \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[1;32m   3094\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3096\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3097\u001b[0m             \u001b[0mis_new_best_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_determine_best_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   3043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3044\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3045\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3046\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4198\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4199\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   4200\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4201\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4383\u001b[0m         \u001b[0;31m# Main evaluation loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4384\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4385\u001b[0m             \u001b[0;31m# Update the observed num examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4386\u001b[0m             \u001b[0mobserved_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    576\u001b[0m                     \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_non_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m                 \u001b[0mnext_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mcurrent_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-97-2749505211.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# Download video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mlocal_video_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_gcs_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgcs_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;31m# Load video with Decord\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-97-2749505211.py\u001b[0m in \u001b[0;36mdownload_gcs_video\u001b[0;34m(gcs_uri, fs)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{bucket}/{path}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNamedTemporaryFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\".mp4\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelete\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m  \u001b[0;31m# return local file path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/tempfile.py\u001b[0m in \u001b[0;36mfunc_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0m_functools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m             \u001b[0;31m# Avoid closing the file as long as the wrapper is alive,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# see issue #18879.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from transformers import VideoMAEModel, VideoMAEImageProcessor, Trainer, TrainingArguments\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "from datetime import datetime\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "class MultiLabelVideoMAE(torch.nn.Module):\n",
        "    def __init__(self, num_labels, label2id, id2label):\n",
        "        super().__init__()\n",
        "        self.backbone = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\")\n",
        "        hidden_size = self.backbone.config.hidden_size\n",
        "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
        "\n",
        "        # Update config\n",
        "        self.config = self.backbone.config\n",
        "        self.config.num_labels = num_labels\n",
        "        self.config.label2id = label2id\n",
        "        self.config.id2label = id2label\n",
        "\n",
        "    def forward(self, pixel_values, labels=None):\n",
        "        outputs = self.backbone(pixel_values=pixel_values)\n",
        "        cls_token = outputs.last_hidden_state[:, 0]  # (batch_size, hidden_size)\n",
        "        logits = self.classifier(cls_token)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fn = nn.BCEWithLogitsLoss()\n",
        "            loss = loss_fn(logits, labels.float())\n",
        "            return {\"loss\": loss, \"logits\": logits}\n",
        "        return {\"logits\": logits}\n",
        "\n",
        "class CheckpointCallback:\n",
        "    \"\"\"Custom callback to save checkpoints to Google Drive\"\"\"\n",
        "\n",
        "    def __init__(self, drive_path=\"/content/drive/MyDrive/videomae_checkpoints\"):\n",
        "        self.drive_path = drive_path\n",
        "        os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "    def on_epoch_end(self, trainer, epoch):\n",
        "        \"\"\"Save checkpoint to Google Drive at the end of each epoch\"\"\"\n",
        "        print(f\"\\nðŸ”„ Saving checkpoint for epoch {epoch + 1}...\")\n",
        "\n",
        "        # Create epoch-specific folder\n",
        "        epoch_folder = os.path.join(self.drive_path, f\"epoch_{epoch + 1}\")\n",
        "        os.makedirs(epoch_folder, exist_ok=True)\n",
        "\n",
        "        # Save the model and tokenizer\n",
        "        trainer.model.save_pretrained(epoch_folder)\n",
        "\n",
        "        # Save training arguments\n",
        "        torch.save(trainer.args, os.path.join(epoch_folder, \"training_args.bin\"))\n",
        "\n",
        "        # Save optimizer and scheduler states\n",
        "        torch.save(trainer.optimizer.state_dict(), os.path.join(epoch_folder, \"optimizer.bin\"))\n",
        "        if trainer.lr_scheduler is not None:\n",
        "            torch.save(trainer.lr_scheduler.state_dict(), os.path.join(epoch_folder, \"scheduler.bin\"))\n",
        "\n",
        "        # Save training metrics if available\n",
        "        if hasattr(trainer.state, 'log_history') and trainer.state.log_history:\n",
        "            torch.save(trainer.state.log_history, os.path.join(epoch_folder, \"training_log.bin\"))\n",
        "\n",
        "        print(f\"âœ… Checkpoint saved to {epoch_folder}\")\n",
        "\n",
        "def setup_training_with_checkpoints():\n",
        "    \"\"\"Setup training with Google Drive checkpoint saving\"\"\"\n",
        "\n",
        "    # Initialize processor and data\n",
        "    processor = VideoMAEImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
        "\n",
        "    # Setup labels (assuming these variables are defined in your environment)\n",
        "    label_columns = valid_labels  # Make sure this is defined\n",
        "    label2id = {label: i for i, label in enumerate(label_columns)}\n",
        "    id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "    # Create datasets (assuming these are defined in your environment)\n",
        "    # train_dataset = VideoMultiLabelDataset(train_df, processor, fs, label_columns)\n",
        "    # val_dataset = VideoMultiLabelDataset(val_df, processor, fs, label_columns)\n",
        "\n",
        "    # Create unique output directory with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    local_output_dir = f\"./videomae-multilabel_{timestamp}\"\n",
        "\n",
        "    # Training arguments with checkpoint configuration\n",
        "    args = TrainingArguments(\n",
        "        output_dir=local_output_dir,\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=2,  # Keep only last 2 checkpoints locally to save space\n",
        "        eval_strategy=\"epoch\",\n",
        "        per_device_train_batch_size=2,\n",
        "        per_device_eval_batch_size=2,\n",
        "        num_train_epochs=5,\n",
        "        learning_rate=2e-5,\n",
        "        remove_unused_columns=False,\n",
        "        logging_steps=10,\n",
        "        logging_dir=f\"{local_output_dir}/logs\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "        dataloader_pin_memory=False,  # Helps with memory issues\n",
        "    )\n",
        "\n",
        "    # Initialize model\n",
        "    model = MultiLabelVideoMAE(\n",
        "        num_labels=len(label_columns),\n",
        "        label2id=label2id,\n",
        "        id2label=id2label\n",
        "    )\n",
        "    train_dataset = VideoMultiLabelJSONDataset(\n",
        "        data = grouped,  # or your split\n",
        "        processor=processor,\n",
        "        label2id=label2id,\n",
        "        fs=fs # Pass the fs object here\n",
        "    )\n",
        "    val_dataset = VideoMultiLabelJSONDataset(\n",
        "        data = grouped, # or your split\n",
        "        processor=processor,\n",
        "        label2id=label2id,\n",
        "        fs=fs # Pass the fs object here\n",
        "    )\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        data_collator=multilabel_collate_fn,  # Make sure this is defined\n",
        "    )\n",
        "\n",
        "    # Initialize checkpoint callback\n",
        "    checkpoint_callback = CheckpointCallback()\n",
        "\n",
        "    return trainer, checkpoint_callback\n",
        "\n",
        "def custom_training_loop(trainer, checkpoint_callback):\n",
        "    \"\"\"Custom training loop with Google Drive checkpointing\"\"\"\n",
        "\n",
        "    print(\"ðŸš€ Starting training with Google Drive checkpoints...\")\n",
        "\n",
        "    # Training loop with manual checkpoint saving\n",
        "    for epoch in range(trainer.args.num_train_epochs):\n",
        "        print(f\"\\nðŸ“ˆ Starting Epoch {epoch + 1}/{trainer.args.num_train_epochs}\")\n",
        "\n",
        "        # Train for one epoch\n",
        "        trainer.train()\n",
        "\n",
        "        # Evaluate\n",
        "        eval_results = trainer.evaluate()\n",
        "        print(f\"ðŸ“Š Epoch {epoch + 1} Evaluation Results: {eval_results}\")\n",
        "\n",
        "        # Save checkpoint to Google Drive\n",
        "        checkpoint_callback.on_epoch_end(trainer, epoch)\n",
        "\n",
        "        # Save final best model to Google Drive\n",
        "        if epoch == trainer.args.num_train_epochs - 1:\n",
        "            final_model_path = os.path.join(checkpoint_callback.drive_path, \"final_best_model\")\n",
        "            os.makedirs(final_model_path, exist_ok=True)\n",
        "            trainer.save_model(final_model_path)\n",
        "            print(f\"ðŸŽ¯ Final best model saved to {final_model_path}\")\n",
        "\n",
        "def load_checkpoint_from_drive(checkpoint_path, model_class, num_labels, label2id, id2label):\n",
        "    \"\"\"Load a checkpoint from Google Drive\"\"\"\n",
        "    print(f\"ðŸ“¥ Loading checkpoint from {checkpoint_path}...\")\n",
        "\n",
        "    # Initialize model\n",
        "    model = model_class(num_labels=num_labels, label2id=label2id, id2label=id2label)\n",
        "\n",
        "    # Load model weights\n",
        "    model.load_state_dict(torch.load(os.path.join(checkpoint_path, \"pytorch_model.bin\")))\n",
        "\n",
        "    # Load optimizer state if needed\n",
        "    optimizer_path = os.path.join(checkpoint_path, \"optimizer.bin\")\n",
        "    if os.path.exists(optimizer_path):\n",
        "        optimizer_state = torch.load(optimizer_path)\n",
        "        print(\"âœ… Optimizer state loaded\")\n",
        "\n",
        "    # Load scheduler state if needed\n",
        "    scheduler_path = os.path.join(checkpoint_path, \"scheduler.bin\")\n",
        "    if os.path.exists(scheduler_path):\n",
        "        scheduler_state = torch.load(scheduler_path)\n",
        "        print(\"âœ… Scheduler state loaded\")\n",
        "\n",
        "    print(\"âœ… Checkpoint loaded successfully!\")\n",
        "    return model\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Setup and run training\n",
        "    trainer, checkpoint_callback = setup_training_with_checkpoints()\n",
        "    custom_training_loop(trainer, checkpoint_callback)\n",
        "\n",
        "    print(\"ðŸŽ‰ Training completed! All checkpoints saved to Google Drive.\")\n",
        "\n",
        "    # Example of loading a checkpoint\n",
        "    # model = load_checkpoint_from_drive(\n",
        "    #     \"/content/drive/MyDrive/videomae_checkpoints/epoch_3\",\n",
        "    #     MultiLabelVideoMAE,\n",
        "    #     len(label_columns),\n",
        "    #     label2id,\n",
        "    #     id2label\n",
        "    # )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "48fda5ccea388be96efda74ea0a13779796a8d8d"
      ],
      "metadata": {
        "id": "ibJeK21zwAug"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "do_y6oEFLrad"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xo24hhtLl-8S"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtuNKgaamCz7"
      },
      "outputs": [],
      "source": [
        "# test on 5\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-J6wADfMmjMx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}